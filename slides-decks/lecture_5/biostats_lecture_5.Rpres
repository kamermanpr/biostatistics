
Lecture 5/6
===========
css: ../custom.css
transition: none
width: 960
height: 720
autosize: false

```{r library_loads, include = FALSE}
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(grid)
library(scales)
library(DiagrammeR)
library(pander)
library(broom)

data("airquality")
data("mtcars")

knitr::opts_chunk$set(echo = FALSE,
               warning = FALSE,
               message = FALSE,
               cache = FALSE,
               fig.width = 10,
               fig.height = 10)
```

## Cookbook of commonly used statistical tests

- Comparing two groups
- Comparing more than two groups

<div style="position:fixed;bottom:10%">
    <h3 style="margin:0;">
        Introduction to Biostatistics
    </h3>
    <p style="font-style:italic;font-size:80%;margin-top:1%;margin-bottom:1%;">
        By: Peter Kamerman &nbsp&nbsp (view at
        <a href="//painblogr.org/biostatistics/" target="_blank">painblogR</a>)
    </p>
    <img src="./resources/cc-by.png" width="128" style="margin:0;"/>
</div>

Recap
=====
title: hide
type: aside

A quick recap on:<br>_p-values & hypothesis testing_

Definition of a p-value
=======================

<div>
    <p style="font-size:150%;font-style:italic;text-align:center;margin-top:60px;">
    "The probability of observing a result as great as (or greater than) you observed if the null hypothesis is true."
    </p>
    <br>
    <p style="text-align:center;">
    If the data are unlikely under the null hypothesis (small p-value), then either we observed a low probability event, <strong>or</strong> it must be that the null hypothesis is not true.<br><br>...only one of these can be correct.
  </p>
</div>

<div class="footer" style="font-size:60%;">
    Source: <a href="http://bayesianbiologist.com/2011/08/21/p-value-fallacy-on-more-or-less/">Corey Chivers, bayesianbiologist</a>
</div>

Hypothesis testing
==================

**Jerzy Neyman and Egon Pearson:**
- Works by setting a threshold $(\alpha)$ that the p-value must cross.

- You state a _null hypothesis_ and an _alternative hypothesis_ and use the _threshold p-value_ as a decision rule.

- The _p-value threshold_ is chosen to control false-positive inference (usually set at $\alpha$ = 0.05).

- You have to abide by the statistical test's 'decision' if you wish to protect against false-positive errors.

Which test?
===========

```{r cheatsheet_1, echo = FALSE, fig.align='center'}
# Save output manually and then import
grViz("
digraph rmarkdown {
# Initialization of node attributes
  node [shape = box,
        fontname = Helvetica,
        color = blue]

# Initialization of edge attributes
  edge [color = black]

# Nodes (using subsitution with footnotes)
a [label = '@@1']
b [label = '@@2-1']
c [label = '@@2-2']
d [label = '@@3-1']
e [label = '@@3-2']
f [label = '@@3-1']
g [label = '@@3-2']
h [label = '@@4']
i [label = '@@5']
j [label = '@@6']
k [label = '@@7']

# Connections
a -> {b c}
b -> {d e}
c -> {f g}
d -> h
e -> i
f -> j
g -> k
}

[1]: '2 groups'
[2]: c('paired', 'unpaired')
[3]: c('parametric', 'non-parametric')
[4]: 'paired\\nt-test'
[5]: 'Wilcoxon\\nsigned-rank\\ntest'
[6]: '(unpaired)\\nt-test'
[7]: 'Wilcoxon\\nrank-sum\\ntest'
", height = 600)
```

Which test?
===========

```{r cheatsheet_2, echo = FALSE, fig.align='center'}
# Save output manually and then import
grViz("
digraph rmarkdown {
# Initialization of node attributes
  node [shape = box,
        fontname = Helvetica,
        color = blue]

# Initialization of edge attributes
  edge [color = black]

# Nodes (using subsitution with footnotes)
a [label = '@@1']
b [label = '@@2-1']
c [label = '@@2-2']
d [label = '@@3-1']
e [label = '@@3-2']
f [label = '@@3-1']
g [label = '@@3-2']
h [label = '@@4']
i [label = '@@5']
j [label = '@@6']
k [label = '@@7']

# Connections
a -> {b c}
b -> {d e}
c -> {f g}
d -> h
e -> i
f -> j
g -> k
}

[1]: '&ge;3 groups'
[2]: c('paired', 'unpaired')
[3]: c('parametric', 'non-parametric')
[4]: 'repeated-measures\\nANOVA'
[5]: 'Friedman\\ntest'
[6]: 'ANOVA'
[7]: 'Kruskal-Wallis\\ntest'
", height = 600)
```

Parametric tests
================

**Experimental groups may differ for two reasons:**

1. Real effect of intervention

2. Random variation between samples drawn from the same population

<h3 style="text-align:center;">You must decide whether:</h3>
<p style="text-align:center;"><bold>[1]</bold> is large enough relative to <bold>[2]</bold> to conclude<br>that a treatment had an effect.</p>

Parametric tests
================

**Calculate the ratio of variances**

1. between-group variance $(\sigma^2_{bet})$

2. within-group variance $(\sigma^2_{with})$

<p style="text-align:center;">If samples are from the same population,<br>the variances will be similar, and...</p>

<p style="text-align:center;">$\frac{\sigma^2_{bet}}{\sigma^2_{with}} \rightarrow 1$</p>

<p style="text-align:center;">Degrees of Freedom <em>(df)</em> determine the critical value the ratio <em>(test statistic)</em> must reach for the null hypothesis to be rejected.</p>

Assumptions for parametric tests
=================================
class: vcenter

- The distribution of the data in the population is _Gaussian_

- Equal variance across groups<br>_(the basis on which the test statistic is calculated)_

- The errors are independent<br>_(the 'error' refers to the difference between each value and the mean)_

- Data are unmatched _(for unpaired data)_ / matching is effective _(for repeated measures data)_

Student's t-test
================
type: twocol

First have a look at the data
```{r t_test, echo = TRUE}
data(sleep)
head(sleep)
```

****
<br>
```{r t_test_2, echo = TRUE, fig.height = 6, fig.width = 6}
boxplot(extra~group, data = sleep)
```

Student's t-test
================
Run _t-test_

```{r t_test_3, echo = TRUE, eval = FALSE}
# When you data are in long format:
t.test(extra ~ group, # formula
       data = sleep,  # data source
       paired = TRUE) # paired or unpaired

# If your data are in wide format,
# you can use the default format:
> t.test(x, y, ...)
```

Student's t-test
================

Output
```{r t_test_4, echo = FALSE}
stt <- t.test(extra ~ group,
              data = sleep,
              paired = TRUE)
stt
```

One-way ANOVA
=============
Use when the grouping factor has $\geq$ 3 levels.

**One-way ANOVA**
```{r 1anova_test, echo = TRUE, eval = FALSE}
foo <- aov(outcome ~ factor_1,
           data = dataframe)
summary(foo)
```

**One-way repeated-measures ANOVA**
```{r 1anova_test_2, echo = TRUE, eval = FALSE}
bar <- aov(outcome ~ factor_2 +
               Error(subjectID), # rep measure
           data = dataframe)
summary(bar)
```

Two-way ANOVA
=============

**Two-way ANOVA**
```{r 2anova_test, echo = TRUE, eval = FALSE}
foo <- aov(outcome ~ factor_1 * factor_2,
           data = dataframe)
summary(foo)

# For main effects only:
> aov(outcome ~ factor_1 + factor_2,...)

# For repeated measures:
> aov(outcome ~ factor_1 + factor_2 +
          Error(subjectID), ...)
```

**NB:** `aov` uses _Type I SS_, so order matters.<br>
**Equivalent to Type III SS:** `drop1(foo, ~., test = 'F')`

Example: One-way ANOVA
======================
type: twocol

```{r anova_example, echo = TRUE}
data("chickwts")
head(chickwts)
```

****

```{r anova_example_2, echo = TRUE, fig.align = 'center', fig.width = 6, fig.height = 6}
boxplot(weight ~ feed,
        data =
            chickwts)
```

Example: One-way ANOVA
======================

```{r anova_example_3, echo = TRUE}
# One-way ANOVA
foo <- aov(weight ~ feed,
           data = chickwts)
summary(foo)
```


Post-hoc tests
==============

**NB:** Need to correct $(\alpha)$ for multiple comparisons<br>to avoid inflation of type I error.

$Bonferroni = \frac{target~p-value}{number~of~comparisons~(n)}$<br><br>
$Holm = \frac{target~p-value}{n - rank~number~in~terms~of ~degree~of~significance + 1}$

```{r post_hoc, echo = TRUE, eval = FALSE}
# Pairwise post-hoc tests
pairwise.t.test(chickwts$weight, chickwts$feed,
                p.adjust.method = 'holm',
                paired = FALSE)

# Several correction methods are available.
```

Post-hoc tests
==============

```{r post_hoc_2, echo = FALSE}
# Pairwise post-hoc tests
pairwise.t.test(chickwts$weight, chickwts$feed,
                p.adjust.method = 'holm',
                paired = FALSE)
```


Post-hoc tests...contd
==============

What if you don't need to compare all pairs?

Pre-planned _(non-orthogonal)_ comparisons
```{r post_hoc_3, echo = TRUE}
# Make a vector of p-values from each of
# the planned comparisons
p <- c('[test 1]' = 0.001, '[test 2]' = 0.211,
       '[test 3]' = 0.013)

# Adjust the p-values accordingly
p.adjust(p,
         method = 'holm')
```

Non-parametric test
===================
Ranking methods

```{r data_rank, echo = FALSE, fig.align = 'center', fig.width = 7, fig.height = 7}
set.seed(1)
foo <- data.frame(x = exp(rnorm(100, mean = 5)),
                  y = rnorm(100))
bar <- foo %>%
    tbl_df() %>%
    mutate(x_ranked = row_number(x)) %>%
    arrange(x_ranked) %>%
    gather(x_group, values, -y) %>%
    select(x_group, values, y) %>%
    group_by(x_group) %>%
    arrange(values)
ggplot(bar, aes(x = values, y = y, fill = x_group)) +
    geom_point(size = 6, shape = 21, colour = '#FFFFFF',
               alpha = 0.7) +
    facet_grid(x_group ~ ., scales = 'free', space = 'free') +
    theme_bw() +
    theme(legend.position = 'none',
          axis.text = element_text(size = 22),
          axis.title = element_text(size = 22),
          strip.text = element_text(size = 22))
```

Assumptions for non-parametric tests
====================================

**Distribution free does not mean assumption free**

- The errors are independent<br>_(the 'error' refers to the difference between each value and the median)_

- Data are unmatched _(for unpaired data)_ / matching is effective _(for repeated measures data)_

- Samples are drawn from populations with the _same shape distributions_.

Non-parametric tests
====================

**Comparing two groups**
```{r non_para, eval = FALSE, echo = TRUE}
# Wilcoxon signed-rank tests (paired t-test)
# Wilcoxon rank-sum test (unpaired t-test)
wilcox.test(value ~ group,
            data = dataframe,
            paired = TRUE)
```

Non-parametric tests
====================

**Comparing more than two groups**
```{r non_para_2, eval = FALSE, echo = TRUE}
# Kruskal-Wallis rank sum test (One-way ANOVA)
kruskal.test(value ~ group,
            data = dataframe)

# Friedman test (Repeated measures ANOVA)
friedman.test(value ~ group | subjectID,
              data = dataframe)
```

Categorical data
================

**Contingency tables**

The relationship between categorical variables can be examined by cross-tabulating the data into contingency tables.

```{r chi, echo = -1}
foo <- data.frame(intervention = sample(rep(c('treated', 'untreated'), each = 100)),
                outcome = sample(c(rep('lived', 80), rep('died', 120))))
# Dummy dataset
head(foo)
```

Categorical data
================
class: center

Cross-tabulate with `table`
```{r chi_2, echo = TRUE}
table(foo$intervention, foo$outcome)
```

Categorical data
================
class: center

Cross-tabulate with `xtabs`
```{r chi_3, echo = TRUE}
xtabs(~intervention + outcome, data = foo)
```

Categorical data
================
class: center

**What if you already have totals?**

```{r chi_4, echo = -1}
bar <- data.frame(Intervention =
                        c('treated', 'untreated', 'treated', 'untreated'),
                    Outcome =
                        c('lived', 'lived', 'died', 'died'),
                    Count = c(43, 37, 57, 63))
print(bar)
```

Categorical data
================
class: center

**...Use `xtabs` with the 'count' on the left of the formula**

```{r chi_5, echo = TRUE}
xtabs(Count~Intervention + Outcome, data = bar)
```

Categorical data
================
class: center

## $\chi^2$ test

Examines whether there is an association between the row variable and the column variable.

_That is, is the distribution of individuals among categories of one variable independent of their distribution among the categories of the other variable?_

Categorical data
================
class: center

The $\chi^2$ test compares the *observed* and the *expected* _(assuming no association)_ frequencies across the row and column variables.

$\chi^2 = \sum\frac{(Observed - Expected)^2}{Expected}$,

where Expected cell frequency $~= \frac{Row~total ~*~Column~total}{Grand~total}$

- If _Observed_ is close to _Expected_, then $\chi^2$ = small

- If _Observed_ is far from _Expected_, then $\chi^2$ = large

Test assumptions
================
class: vcenter

- Random sampling

- Observations are independent _(unpaired)_

- Large sample, with adequate _expected_ cell counts
    - 2 x 2 table: Expected $\geq$ 5 in all cells;
    - $\geq$ 2 x 3 table: Expected $\geq$ 5 in 80% of cells;
    - Expected $\neq$ 0 in any cell.

- Assumes the discrete probability of observed frequencies in the table can be _approximated_ by the continuous $\chi^2$ distribution.

Solutions
=========
class: vcenter

- At low sample sizes, p-values are too low _(risk of type I error)_, so introduce a p-value correction: **Yates' Correction for Continuity**.

- $\chi^2_{Yates} = \sum\frac{(|Observed - Expected|~- \frac{1}{2})^2}{Expected}$

- ...but Yates' correction may overcorrect the p-value and increase risk of Type II errors

Solutions
=========
class: vcenter

- **Fisher's Exact Test**.

- Doesn't use an approximation of the  $\chi^2$.

- _Exact_ p-values are calculated.

- Computationally intense, therefore traditionally limited to 2 x 2 tables.

- _R_ provides mechanisms to larger tables.

Examples
========

```{r chi_6, echo = 3:4}
baz <- xtabs(~ intervention + outcome, data = foo)
baz
chisq.test(baz, correct = FALSE)
# TRUE for Yates' correction
```

Examples
========
class: center

```{r chi_7, echo = 3:4}
baz <- xtabs(~ intervention + outcome, data = foo)
fisher.test(baz)
```

Examples
========

**What about bigger tables?**
```{r chi_8, echo = 4}
qux <- data.frame(Sex = rep(c('Male', 'Female'), times = 3),
                  Blood_group = c('A', 'A', 'B', 'B', 'O', 'O'),
                  Count = c(15, 20, 30, 13, 20, 10))
fred <- xtabs(Count ~ Sex + Blood_group, data = qux)
fred
fisher.test(fred)

```

Examples
========
class: center

**Divide and conquer bigger tables**
```{r chi_9, echo = 4}
barney <- data.frame(Sex = rep(c('Male', 'Female'), times = 2),
                  Blood_group = c('A', 'A', 'B', 'B'),
                  Count = c(15, 20, 30, 13))
waldo <- xtabs(Count ~ Sex + Blood_group, data = barney)
waldo
tidy(# broom::tidy results to save space
    fisher.test(waldo) # A vs B blood group
    )

```

Examples
========
class: center

**Divide and conquer bigger tables**
```{r chi_10, echo = 4}
foo <- data.frame(Sex = rep(c('Male', 'Female'), times = 2),
                  Blood_group = c('A', 'A', 'O', 'O'),
                  Count = c(15, 20, 20, 10))
bar <- xtabs(Count ~ Sex + Blood_group, data = foo)
bar
tidy(
    fisher.test(bar) # A vs O blood group
    )
```

Examples
========
class: center

**Divide and conquer bigger tables**
```{r chi_11, echo = 4}
baz <- data.frame(Sex = rep(c('Male', 'Female'), times = 2),
                  Blood_group = c('B', 'B', 'O', 'O'),
                  Count = c(30, 13, 20, 10))
qux <- xtabs(Count ~ Sex + Blood_group, data = baz)
qux
tidy(
    fisher.test(qux) # A vs O blood group
    )
```

Examples
========
class: center

**Multiple non-orthogonal tests, so need $\alpha$ correction**
```{r chi_12, echo = TRUE}
AB <- fisher.test(waldo)$p.value # A vs B
AO <- fisher.test(bar)$p.value # A vs O
BO <- fisher.test(qux)$p.value # B vs O

p.adjust(c(AB, AO, BO), method = 'holm')
```

Ordinal data
============

**Order has meaning, so use it**
```{r chi_13, echo = 4}
baz <- data.frame(Outcome = rep(c('Lived', 'Died'), times = 3),
                  Disease_severity =
                      c('I', 'I', 'II', 'II', 'III', 'III'),
                  Count = c(30, 13, 20, 10, 5, 30))
qux <- xtabs(Count ~ Outcome + Disease_severity, data = baz)
qux
# install and load vcdExtra package
```

Ordinal data
============

**Order has meaning, so use it**
```{r chi_14, echo = 2:5}
library(vcdExtra)
# Run Cochran-Mantel-Haenszel Tests
# Assign scores to the ordered rows (rscores)
# or columns (cscores)
CMHtest(qux,
        types = 'cmeans', # compare columns
        cscores = c(1, 2, 3)) # column scores
```

What is my data are paired?
==========================

**Use the McNemar Test**
```{r chi_15, echo = -c(1:2)}
Performance <- matrix(c(794, 86, 150, 570),
                      nrow = 2,
                      dimnames =
                          list("1st Survey" = c("Approve", "Disapprove"),
                               "2nd Survey" = c("Approve", "Disapprove")))
Performance
mcnemar.test(Performance)
```

Assignment
==========
class: vcenter

Practice applying the significance tests you learned about in this lecture by completing [assignment 4](https://painblogr.org/biostatistics#assignments).

The assignment also gives you a chance to practice using skills acquired in earlier lessions (_rmarkdown_, _git_, _GitHub_, _data cleaning_, and _plotting_).


Web resources
=============
class: vcenter

_**Basic statistics**_
- Quick-R: [basic statistics](http://www.statmethods.net/stats/index.html), by Robert Kabacoff.

- r-statistics.co: [statistical tests](http://r-statistics.co/Statistical-Tests-in-R.html), by Selva Prabhakaran


tl;dr
===================================

<div class="center", style="width:80%;">
    <p style="font-size:150%;font-style:italic;text-align:center;margin-top:60px;">
    "But the shocking discovery was that 50% were below the median age."
    </p>
    <p style="text-align:center">
     Dogbert<br>
        <span style="font-style:italic;font-size:80%;">(megalomaniac alter ego of Dilbert, by Scott Adams)</span>
    </p>
</div>
